{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PyTorch Dataset\n",
    "Time to refresh your PyTorch Datasets knowledge!\n",
    "\n",
    "Before model training can commence, you need to load the data and pass it to the model in the right format. In PyTorch, this is handled by Datasets and DataLoaders. Let's start with building a PyTorch Dataset for our water potability data.\n",
    "\n",
    "In this exercise, you will define a class called WaterDataset to load the data from a CSV file. To do this, you will need to implement the three methods which PyTorch expects a Dataset to have:\n",
    "\n",
    ".__init__() to load the data,\n",
    ".__len__() to return data size,\n",
    ".__getitem()__ to extract features and label for a single sample."
   ],
   "id": "efbe215d638ce36c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:58:46.377724Z",
     "start_time": "2024-06-14T19:58:44.276526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__()\n",
    "        # Load the data to pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Convert the DataFrame to numpy array and assingn it to self.data\n",
    "        self.data = df.to_numpy()\n",
    "        \n",
    "# Implement the .__len__() method to return the number of data samples.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "# In the .__getitem__() method, get the label by slicing self.data to extract its last column for the index idx, similarly to how it's done for the features.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1]\n",
    "        label = self.data[idx, -1]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
   ],
   "id": "50c4908d190abc40",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:58:54.927768Z",
     "start_time": "2024-06-14T19:58:54.889808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the WaterDataset class to load the data.\n",
    "\n",
    "# Create an instance of the WaterDataset\n",
    "dataset = WaterDataset('/data/raw_data/water_potability/water_train.csv')\n",
    "\n",
    "# Get the length of the dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# Get the first sample from the dataset\n",
    "first_sample = dataset[0]\n",
    "print(first_sample)"
   ],
   "id": "6fe3dc7eecc045df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508\n",
      "(tensor([0.4836, 0.6156, 0.5140, 0.7774, 0.3546, 0.3353, 0.3673, 0.5141, 0.6173]), tensor(1.))\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next step in preparing the training data is to set up a DataLoader. A PyTorch DataLoader can be created from a Dataset to load data, split it into batches, and perform transformations on the data if desired. Then, it yields a data sample ready for training.\n",
    "\n",
    "In this exercise, you will build a DataLoader based on the WaterDataset. The DataLoader class you will need has already been imported for you from torch.utils.data. Let's get to it!\n",
    "\n",
    "Instrucciones\n",
    "100 XP\n",
    "Create an instance of WaterDataset from water_train.csv, assigning it to dataset_train.\n",
    "Create dataloader_train based on dataset_train, using a batch size of two and shuffling the samples.\n",
    "Get a batch of features and labels from the DataLoader and print them."
   ],
   "id": "5e48cbfde2143b36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:59:11.986188Z",
     "start_time": "2024-06-14T19:59:11.972207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an instance of the WaterDataset\n",
    "dataset_train = WaterDataset('water_train.csv')\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=2, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get a batch of data\n",
    "features, labels = next(iter(dataloader_train))\n",
    "print(features, labels)"
   ],
   "id": "31463cc411e39da9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6134, 0.5218, 0.2622, 0.6169, 0.6471, 0.4344, 0.6310, 0.5719, 0.6995],\n",
      "        [0.5452, 0.6216, 0.3414, 0.4914, 0.4572, 0.4211, 0.5222, 0.5734, 0.4462]]) tensor([0., 1.])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PyTorch Model\n",
    "You will use the OOP approach to define the model architecture. Recall that this requires setting up a model class and defining two methods inside it:\n",
    "\n",
    ".__init__(), in which you define the layers you want to use;\n",
    "\n",
    "forward(), in which you define what happens to the model inputs once it receives them; this is where you pass inputs through pre-defined layers.\n",
    "\n",
    "Let's build a model with three linear layers and ReLU activations. After the last linear layer, you need a sigmoid activation instead, which is well-suited for binary classification tasks like our water potability prediction problem. Here's the model defined using nn.Sequential(), which you may be more familiar with:\n",
    "\n",
    "net = nn.Sequential(\n",
    "  nn.Linear(9, 16),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(16, 8),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid(),\n",
    ")\n",
    "Let's rewrite this model as a class!"
   ],
   "id": "951572de34db2498"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the .__init__() method, define the three linear layers with dimensions corresponding to the model definition provided and assign them to self.fc1, self.fc2, and self.fc3, respectively.\n",
    "In the forward() method, pass the model input x through all the layers, remembering to add activations on top of them, similarly how it's already done for the first layer."
   ],
   "id": "9057c8815f251f5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T20:03:01.746165Z",
     "start_time": "2024-06-14T20:03:01.740929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass x through linear layers adding activations\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ],
   "id": "1c9f2fb949463a4a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T20:03:22.582375Z",
     "start_time": "2024-06-14T20:03:22.576386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an instance of the Net class\n",
    "model = Net()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Get the model parameters\n",
    "print(dict(model.named_parameters()))\n",
    "\n",
    "# Get the model parameters and their shapes\n",
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ],
   "id": "80279ff1d740c821",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=9, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "{'fc1.weight': Parameter containing:\n",
      "tensor([[ 0.1922, -0.0217,  0.0400, -0.1087, -0.0751, -0.1268,  0.3267, -0.0669,\n",
      "         -0.3015],\n",
      "        [-0.2916,  0.2606, -0.2145, -0.1060,  0.1204, -0.0006, -0.0482,  0.1936,\n",
      "         -0.1853],\n",
      "        [-0.0603,  0.0081,  0.3327,  0.0040, -0.2450, -0.0972, -0.0151, -0.0382,\n",
      "          0.0849],\n",
      "        [-0.2542,  0.3188, -0.3275,  0.1676,  0.1902, -0.1359, -0.2198, -0.0926,\n",
      "         -0.0491],\n",
      "        [-0.3257, -0.0137, -0.2639,  0.2183,  0.2283,  0.1144,  0.1347,  0.3025,\n",
      "         -0.1006],\n",
      "        [-0.2558, -0.1875, -0.2830, -0.0660, -0.0623,  0.1309, -0.2063, -0.3026,\n",
      "          0.1782],\n",
      "        [ 0.1301,  0.0056, -0.1785,  0.1608, -0.2156,  0.3099, -0.0148,  0.1713,\n",
      "          0.2356],\n",
      "        [-0.2412,  0.0477,  0.0822,  0.0411, -0.1899,  0.0455, -0.0209,  0.0130,\n",
      "         -0.1529],\n",
      "        [ 0.0143,  0.2273,  0.1422, -0.1734,  0.0613,  0.1047,  0.3138, -0.2503,\n",
      "         -0.0858],\n",
      "        [-0.0711,  0.2033, -0.1989,  0.1266,  0.0357, -0.0757,  0.0099, -0.2430,\n",
      "          0.3051],\n",
      "        [ 0.1639,  0.1711,  0.0945, -0.2681, -0.1368,  0.0543,  0.1910, -0.3048,\n",
      "         -0.2731],\n",
      "        [ 0.3065, -0.0738, -0.1221,  0.3223,  0.2886, -0.0829,  0.1828, -0.2452,\n",
      "          0.1660],\n",
      "        [-0.3084, -0.2873, -0.1338, -0.2795,  0.1811,  0.2535, -0.1710,  0.0710,\n",
      "         -0.2643],\n",
      "        [ 0.2072, -0.3036, -0.1452, -0.1888, -0.3280,  0.0701, -0.3143,  0.2981,\n",
      "         -0.0565],\n",
      "        [ 0.2592, -0.2396,  0.1321, -0.2675, -0.1187,  0.1930,  0.2863, -0.3095,\n",
      "         -0.2021],\n",
      "        [ 0.1831,  0.1781, -0.1073,  0.0802, -0.2714,  0.1921,  0.2491,  0.1268,\n",
      "          0.1486]], requires_grad=True), 'fc1.bias': Parameter containing:\n",
      "tensor([ 0.1123, -0.1531, -0.0466,  0.2102,  0.0713, -0.1648,  0.2017, -0.1240,\n",
      "        -0.1766, -0.3254,  0.1868, -0.0473,  0.1569, -0.2413, -0.0702, -0.2510],\n",
      "       requires_grad=True), 'fc2.weight': Parameter containing:\n",
      "tensor([[ 0.1812,  0.2390, -0.1576, -0.0923, -0.0685, -0.0085, -0.0457, -0.0473,\n",
      "          0.2212,  0.0108, -0.0214, -0.2442,  0.1441,  0.1048, -0.1246,  0.0615],\n",
      "        [-0.1159, -0.0203,  0.0201,  0.0069,  0.0893, -0.2372,  0.2038,  0.1545,\n",
      "          0.0698, -0.0579,  0.1442, -0.0116, -0.1893,  0.0782,  0.1775,  0.2299],\n",
      "        [-0.0703,  0.0183, -0.1340,  0.1296, -0.0157,  0.2005,  0.0447, -0.1097,\n",
      "          0.0335, -0.0391, -0.1014, -0.1585, -0.1204, -0.1887,  0.0135, -0.1150],\n",
      "        [-0.0960,  0.2161,  0.2412,  0.0165, -0.0764, -0.1864,  0.1631,  0.0330,\n",
      "          0.1967, -0.1744, -0.0102, -0.0661,  0.0503, -0.2492, -0.1713, -0.1426],\n",
      "        [ 0.0014,  0.1135, -0.1453, -0.1347, -0.2382,  0.1529, -0.1402, -0.0871,\n",
      "          0.0163, -0.1280,  0.1549, -0.1026, -0.0039, -0.1857, -0.2126,  0.2001],\n",
      "        [-0.1221, -0.0074,  0.2313, -0.1086, -0.2393,  0.1323, -0.1829, -0.0136,\n",
      "          0.0330,  0.2481, -0.1215,  0.0960, -0.1856, -0.1692,  0.2300,  0.1449],\n",
      "        [ 0.0400, -0.1246, -0.0824, -0.1059,  0.1794,  0.1117, -0.0087,  0.1871,\n",
      "         -0.2337,  0.0272,  0.0195,  0.0254,  0.1842, -0.0390, -0.2363,  0.0631],\n",
      "        [-0.0501,  0.1418,  0.1228, -0.1207, -0.0035,  0.1878,  0.1837,  0.1272,\n",
      "          0.0935, -0.0107, -0.0174, -0.0877, -0.1172,  0.0510,  0.1254,  0.1724]],\n",
      "       requires_grad=True), 'fc2.bias': Parameter containing:\n",
      "tensor([ 0.1912, -0.0469, -0.2082, -0.1158,  0.2025,  0.2157, -0.0183,  0.2110],\n",
      "       requires_grad=True), 'fc3.weight': Parameter containing:\n",
      "tensor([[-0.1491,  0.2707,  0.1631,  0.3260, -0.3471, -0.0317,  0.3429,  0.2844]],\n",
      "       requires_grad=True), 'fc3.bias': Parameter containing:\n",
      "tensor([-0.0519], requires_grad=True)}\n",
      "torch.Size([16, 9])\n",
      "torch.Size([16])\n",
      "torch.Size([8, 16])\n",
      "torch.Size([8])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Optimizers\n",
    "It's time to explore the different optimizers that you can use for training your model.\n",
    "\n",
    "A custom function called train_model(optimizer, net, num_epochs) has been defined for you. It takes the optimizer, the model, and the number of epochs as inputs, runs the training loops, and prints the training loss at the end.\n",
    "\n",
    "Let's use train_model() to run a few short trainings with different optimizers and compare the results!"
   ],
   "id": "5af33639c8ca7df7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the optimizer as Stochastic Gradient Descent.",
   "id": "fe46acfc28ce941f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(optimizer, model, num_epochs=5)"
   ],
   "id": "3aa7891ab0b7ae17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the optimizer as Root Mean Square Propagation (RMSprop), passing the model's parameters as its first argument.",
   "id": "1dde43eea15cf723"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)"
   ],
   "id": "f117b51cb6600b0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define the optimizer as Adaptive Moments Estimation (Adam), setting the learning rate to 0.001.",
   "id": "603fc769f85f436e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "e009d9eb81aa1b43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model evaluation\n",
    "With the training loop sorted out, you have trained the model for 1000 epochs, and it is available to you as net. You have also set up a test_dataloader in exactly the same way as you did with train_dataloader before—just reading the data from the test rather than the train directory.\n",
    "\n",
    "You can now evaluate the model on test data. To do this, you will need to write the evaluation loop to iterate over the batches of test data, get the model's predictions for each batch, and calculate the accuracy score for it. Let's do it!\n",
    "\n",
    "Instrucciones\n",
    "100 XP\n",
    "Set up the evaluation metric as Accuracy for binary classification and assign it to acc.\n",
    "For each batch of test data, get the model's outputs and assign them to outputs.\n",
    "After the loop, compute the total test accuracy and assign it to test_accuracy."
   ],
   "id": "af0652933926f9e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Set up binary accuracy metric\n",
    "acc = Accuracy(\"binary\")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        # Get predicted probabilities for test data batch\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc.update(preds, labels.view(-1, 1))\n",
    "\n",
    "# Compute total test accuracy\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ],
   "id": "8b05d9a26336c2b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialization and activation\n",
    "The problems of unstable (vanishing or exploding) gradients are a challenge that often arises in training deep neural networks. In this and the following exercises, you will expand the model architecture that you built for the water potability classification task to make it more immune to those problems.\n",
    "\n",
    "As a first step, you'll improve the weights initialization by using He (Kaiming) initialization strategy. To do so, you will need to call the proper initializer from the torch.nn.init module, which has been imported for you as init. Next, you will update the activations functions from the default ReLU to the often better ELU."
   ],
   "id": "3cb38f82108a1950"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Call the He (Kaiming) initializer on the weight attribute of the second layer, fc2, similarly to how it's done for fc1.\n",
    "Call the He (Kaiming) initializer on the weight attribute of the third layer, fc3, accounting for the different activation function used in the final layer.\n",
    "Update the activation functions in the forward() method from relu to elu."
   ],
   "id": "bdec43604fc7ccab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "        # Apply He initialization\n",
    "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update ReLU activation to ELU\n",
    "        x = nn.functional.elu(self.fc1(x))\n",
    "        x = nn.functional.elu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ],
   "id": "f1952e9ae40a5a47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Batch Normalization\n",
    "As a final improvement to the model architecture, let's add the batch normalization layer after each of the two linear layers. The batch norm trick tends to accelerate training convergence and protects the model from vanishing and exploding gradients issues.\n",
    "\n",
    "Both torch.nn and torch.nn.init have already been imported for you as nn and init, respectively. Once you implement the change in the model architecture, be ready to answer a short question on how batch normalization works!\n",
    "\n",
    "Instrucciones 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Add two BatchNorm1d layers assigning them to self.bn1 and self.bn2."
   ],
   "id": "278746edb9b8480e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "        # Add two batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")"
   ],
   "id": "f9df3a32ac4416ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the forward() method, pass x through the second set of layers: the linear layer, the batch norm layer, and the activations, similarly to how it's done for the first set of layers.",
   "id": "f6c5d6be2e1b2472"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "\n",
    "        # Add two batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        # Pass x through the second set of layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ],
   "id": "65bdca83fa5b9af6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
